{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>switchfoot httptwitpiccom2y1zl awww bummer sho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>upset cannot update facebook texting might cry...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kenichan dived many time ball managed save 50 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nationwideclass behaving mad cannot see</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>woke school best feeling ever</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>thewdbcom cool hear old walt interview ♫ httpb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>ready mojo makeover ask detail</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>happy 38th birthday boo alll time tupac amaru ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>happy charitytuesday thenspcc sparkscharity sp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599626 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text_lemmatized  sentiment\n",
       "0        switchfoot httptwitpiccom2y1zl awww bummer sho...          0\n",
       "1        upset cannot update facebook texting might cry...          0\n",
       "2        kenichan dived many time ball managed save 50 ...          0\n",
       "3                          whole body feel itchy like fire          0\n",
       "4                  nationwideclass behaving mad cannot see          0\n",
       "...                                                    ...        ...\n",
       "1599995                      woke school best feeling ever          1\n",
       "1599996  thewdbcom cool hear old walt interview ♫ httpb...          1\n",
       "1599997                     ready mojo makeover ask detail          1\n",
       "1599998  happy 38th birthday boo alll time tupac amaru ...          1\n",
       "1599999  happy charitytuesday thenspcc sparkscharity sp...          1\n",
       "\n",
       "[1599626 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('preprocessed_data.csv')\n",
    "data = data[['text_lemmatized','sentiment']]\n",
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text_lemmatized\"],data[\"sentiment\"],test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mdzak\\anaconda3\\envs\\irproject\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_vectors_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78    160141\n",
      "           1       0.77      0.80      0.79    159785\n",
      "\n",
      "    accuracy                           0.78    319926\n",
      "   macro avg       0.78      0.78      0.78    319926\n",
      "weighted avg       0.78      0.78      0.78    319926\n",
      "\n",
      "Confusion Matrix: [[122858  37283]\n",
      " [ 31852 127933]]\n",
      "AUC: 0.8632340906474503\n"
     ]
    }
   ],
   "source": [
    "y_predict = lr.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1396042</th>\n",
       "      <td>mikachu84 absolutely bad weekend thats lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66973</th>\n",
       "      <td>obnoxiousacorns afraid twittered everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515888</th>\n",
       "      <td>well done braving 14 degree sognsvann nordmark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364648</th>\n",
       "      <td>anyone like 1985 movie phenomenonback future a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398961</th>\n",
       "      <td>homework</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560425</th>\n",
       "      <td>hello woke accompanying mum go hospital check ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110326</th>\n",
       "      <td>halfacanyon two week tomorrow must write devol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855147</th>\n",
       "      <td>ashsimpsonwentz gnight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505661</th>\n",
       "      <td>lakers ot went exactly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575075</th>\n",
       "      <td>ronewmedia today hope useful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319926 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text_lemmatized\n",
       "1396042         mikachu84 absolutely bad weekend thats lol\n",
       "66973          obnoxiousacorns afraid twittered everything\n",
       "1515888  well done braving 14 degree sognsvann nordmark...\n",
       "364648   anyone like 1985 movie phenomenonback future a...\n",
       "398961                                            homework\n",
       "...                                                    ...\n",
       "1560425  hello woke accompanying mum go hospital check ...\n",
       "110326   halfacanyon two week tomorrow must write devol...\n",
       "855147                              ashsimpsonwentz gnight\n",
       "1505661                             lakers ot went exactly\n",
       "1575075                       ronewmedia today hope useful\n",
       "\n",
       "[319926 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cases = X_test.to_frame()\n",
    "test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>predict_prob</th>\n",
       "      <th>predicted sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1396042</th>\n",
       "      <td>mikachu84 absolutely bad weekend thats lol</td>\n",
       "      <td>0.493538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66973</th>\n",
       "      <td>obnoxiousacorns afraid twittered everything</td>\n",
       "      <td>0.253162</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515888</th>\n",
       "      <td>well done braving 14 degree sognsvann nordmark...</td>\n",
       "      <td>0.684442</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364648</th>\n",
       "      <td>anyone like 1985 movie phenomenonback future a...</td>\n",
       "      <td>0.879123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398961</th>\n",
       "      <td>homework</td>\n",
       "      <td>0.097314</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560425</th>\n",
       "      <td>hello woke accompanying mum go hospital check ...</td>\n",
       "      <td>0.568867</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110326</th>\n",
       "      <td>halfacanyon two week tomorrow must write devol...</td>\n",
       "      <td>0.344660</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855147</th>\n",
       "      <td>ashsimpsonwentz gnight</td>\n",
       "      <td>0.931916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505661</th>\n",
       "      <td>lakers ot went exactly</td>\n",
       "      <td>0.676502</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575075</th>\n",
       "      <td>ronewmedia today hope useful</td>\n",
       "      <td>0.802485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319926 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text_lemmatized  predict_prob  \\\n",
       "1396042         mikachu84 absolutely bad weekend thats lol      0.493538   \n",
       "66973          obnoxiousacorns afraid twittered everything      0.253162   \n",
       "1515888  well done braving 14 degree sognsvann nordmark...      0.684442   \n",
       "364648   anyone like 1985 movie phenomenonback future a...      0.879123   \n",
       "398961                                            homework      0.097314   \n",
       "...                                                    ...           ...   \n",
       "1560425  hello woke accompanying mum go hospital check ...      0.568867   \n",
       "110326   halfacanyon two week tomorrow must write devol...      0.344660   \n",
       "855147                              ashsimpsonwentz gnight      0.931916   \n",
       "1505661                             lakers ot went exactly      0.676502   \n",
       "1575075                       ronewmedia today hope useful      0.802485   \n",
       "\n",
       "         predicted sentiment  \n",
       "1396042                    0  \n",
       "66973                      0  \n",
       "1515888                    1  \n",
       "364648                     1  \n",
       "398961                     0  \n",
       "...                      ...  \n",
       "1560425                    1  \n",
       "110326                     0  \n",
       "855147                     1  \n",
       "1505661                    1  \n",
       "1575075                    1  \n",
       "\n",
       "[319926 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tfidf = tfidf_vectorizer.transform(test_cases['text_lemmatized'])\n",
    "prediction = lr.predict(test_tfidf)\n",
    "prediction_prob = lr._predict_proba_lr(test_tfidf)[:,1]\n",
    "\n",
    "test_cases['predict_prob'] = prediction_prob\n",
    "test_cases['predicted sentiment'] = prediction\n",
    "\n",
    "test_cases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on single text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"love the people\"\n",
    "text_array = []\n",
    "text_array.append(text)\n",
    "text_tfidf = tfidf_vectorizer.transform(text_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(text_array, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lr.predict(text_tfidf)\n",
    "y_prob = lr.predict_proba(text_tfidf)[:,1]\n",
    "test['pred sentiment'] = y\n",
    "test['predict prob'] = y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pred sentiment</th>\n",
       "      <th>predict prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love the people</td>\n",
       "      <td>1</td>\n",
       "      <td>0.889488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text  pred sentiment  predict prob\n",
       "0  love the people               1      0.889488"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab2d501dfb183ce2a9e8c1cd90d52d22b39c236a56c92044160ebf998ad3b3c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
